{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQtXuaaUz0lv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7fe7df3-142e-48cf-92f8-8af3a9a333e7"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "import json\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "YUZDhCJL500P",
        "outputId": "dad8e6c4-4e19-4ab0-f672-937a32a84abd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('drive/MyDrive/NLP_assignment2_data')\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/NLP_assignment2_data'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w88KjZt5Z-1"
      },
      "source": [
        "train=open(\"train.txt\",\"r\")\n",
        "train=train.read()\n",
        "train=train.lower()\n",
        "\n",
        "sentences=train.split('\\n')     #considering a sentence a full line until \\n\n",
        "vocabulary=[]\n",
        "unigram_count={}                #storing the words and their frequency\n",
        "bigram_count={}                 #storing bigrams as tuples for example I am is stored as ('i','am'), and their frequency"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2bN9sNr7776",
        "outputId": "f306d9a0-8053-4b8d-acab-60339c9320b6"
      },
      "source": [
        "print(len(sentences))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2206502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTxxhr6l_cOW"
      },
      "source": [
        "fraction_used=1                 #the fraction of total sentences being used\n",
        "index=int(fraction_used*len(sentences))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfM48Z-SOMnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "871828f4-5277-438a-feb3-695ee8235351"
      },
      "source": [
        "sentences_using=sentences[:index]\n",
        "\n",
        "for i in range(len(sentences_using)):\n",
        "  words=nltk.word_tokenize(sentences_using[i])\n",
        "  if (i%50000==0):\n",
        "    print(\"Processed \"+str(i))\n",
        "  for j in words:\n",
        "    if j in unigram_count:\n",
        "      unigram_count[j]+=1\n",
        "    else:\n",
        "      unigram_count[j]=1\n",
        "\n",
        "  for j in range(len(words)-1):\n",
        "    if (words[j],words[j+1]) in bigram_count:\n",
        "      bigram_count[(words[j],words[j+1])]+=1\n",
        "    else:\n",
        "      bigram_count[(words[j],words[j+1])]=1\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 0\n",
            "Processed 50000\n",
            "Processed 100000\n",
            "Processed 150000\n",
            "Processed 200000\n",
            "Processed 250000\n",
            "Processed 300000\n",
            "Processed 350000\n",
            "Processed 400000\n",
            "Processed 450000\n",
            "Processed 500000\n",
            "Processed 550000\n",
            "Processed 600000\n",
            "Processed 650000\n",
            "Processed 700000\n",
            "Processed 750000\n",
            "Processed 800000\n",
            "Processed 850000\n",
            "Processed 900000\n",
            "Processed 950000\n",
            "Processed 1000000\n",
            "Processed 1050000\n",
            "Processed 1100000\n",
            "Processed 1150000\n",
            "Processed 1200000\n",
            "Processed 1250000\n",
            "Processed 1300000\n",
            "Processed 1350000\n",
            "Processed 1400000\n",
            "Processed 1450000\n",
            "Processed 1500000\n",
            "Processed 1550000\n",
            "Processed 1600000\n",
            "Processed 1650000\n",
            "Processed 1700000\n",
            "Processed 1750000\n",
            "Processed 1800000\n",
            "Processed 1850000\n",
            "Processed 1900000\n",
            "Processed 1950000\n",
            "Processed 2000000\n",
            "Processed 2050000\n",
            "Processed 2100000\n",
            "Processed 2150000\n",
            "Processed 2200000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC-6v9zlKi-3"
      },
      "source": [
        "for i in unigram_count:\n",
        "  vocabulary.append(i)        #adding all words in vocabulary"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSvy6O7QKugq"
      },
      "source": [
        "validation=[]                 #list of all questions is stored as list of dictionary\n",
        "with open('validation.json') as f:\n",
        "  for i in f:\n",
        "    validation.append(json.loads(i))        "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddfck3to-ZYJ"
      },
      "source": [
        "file1 = open(\"without_bonus_output.txt\",\"a\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS-3TQMiK_eq"
      },
      "source": [
        "def without_bonus_smoothing(type_smoothing,validation):\n",
        "  \"\"\" \n",
        "      given a question -> word1 word2 xxxxx word3\n",
        "      type_smoothing=1 is for no smoothing\n",
        "      predicting the bigram probability of (xxxxx|word2) = bigram_count(word2 xxxxx)/unigram_count(word2)\n",
        "\n",
        "      type_smoothing=2 is for add 1 smoothing\n",
        "      predicting the bigram probability of (xxxxx|word2) = (1+bigram_count(word2 xxxxx))/(unigram_count(word2)+length_vocabulary)\n",
        "\n",
        "      type_smoothing=1 is for add k smoothing\n",
        "      predicting the bigram probability of (xxxxx|word2) = (k+bigram_count(word2 xxxxx))/(unigram_count(word2)+k*length_vocabulary)\n",
        "\n",
        "  \"\"\"\n",
        "  correct=0\n",
        "  k=100    #for add k smoothing\n",
        "\n",
        "  if(type_smoothing==1):\n",
        "    file1.write(\"without smoothing \\n\")\n",
        "  elif(type_smoothing==2):\n",
        "    file1.write(\"add 1 smoothing \\n\")\n",
        "  else:\n",
        "    file1.write(\"add k smoothing \\n\")\n",
        "\n",
        "  \n",
        "  for i in validation:\n",
        "    question=nltk.word_tokenize(i['question'].lower())\n",
        "    xxxxx_index=question.index('xxxxx')           \n",
        "    before=''   \n",
        "\n",
        "    if(xxxxx_index==0):\n",
        "      continue\n",
        "    \n",
        "    before=question[xxxxx_index-1]\n",
        "\n",
        "    prob=0\n",
        "    optimal_ans=''\n",
        "\n",
        "    if(len(before)!=0):\n",
        "      for j in i['options']:\n",
        "        j=j.lower()\n",
        "        to_check=(before,j)\n",
        "\n",
        "        if(to_check not in bigram_count):\n",
        "            bigram_count[to_check]=0\n",
        "\n",
        "        if(type_smoothing!=1):\n",
        "          if(before not in unigram_count):      #in case of no smoothing division by 0 error can occur if the case is not handled\n",
        "            unigram_count[before]=0\n",
        "\n",
        "        \n",
        "        if(type_smoothing==1 and (before in unigram_count)):\n",
        "          if(bigram_count[to_check]/unigram_count[before]>prob):      #considering the one with maxmimum probability\n",
        "            prob=bigram_count[to_check]/unigram_count[before]         #without smoothing\n",
        "            optimal_ans=j\n",
        "\n",
        "        elif(type_smoothing==2):\n",
        "          if((bigram_count[to_check]+1)/(unigram_count[before]+len(vocabulary))>prob):    #probability with add 1 smoothing\n",
        "            prob=(bigram_count[to_check]+1)/(unigram_count[before]+len(vocabulary))\n",
        "            optimal_ans=j\n",
        "\n",
        "        elif(type_smoothing==3):\n",
        "          if((bigram_count[to_check]+k)/(unigram_count[before]+k*len(vocabulary))>prob):   #probability with add k smoothing\n",
        "            prob=(bigram_count[to_check]+k)/(unigram_count[before]+k*len(vocabulary))\n",
        "            optimal_ans=j\n",
        "    \n",
        "    if(optimal_ans==i['answer']):\n",
        "      correct+=1\n",
        "\n",
        "    output=\"correct ans : \"+i['answer'].lower()+\", prediction is: \"+optimal_ans + \"\\n\"\n",
        "    file1.write(output)\n",
        "\n",
        "  print(\"Accuracy with type\"+str(type_smoothing)+\" smoothing is \"+str(100*correct/len(validation))+\" %\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11FyfFaRTLfY",
        "outputId": "a0192ee2-e204-4183-8d48-8b89850f833f"
      },
      "source": [
        "without_bonus_smoothing(1,validation)\n",
        "without_bonus_smoothing(2,validation)\n",
        "without_bonus_smoothing(3,validation)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with type1 smoothing is 51.2 %\n",
            "Accuracy with type2 smoothing is 51.2 %\n",
            "Accuracy with type3 smoothing is 51.2 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHu88swtAzy2"
      },
      "source": [
        "file2 = open(\"with_bonus_output.txt\",\"a\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu0ie9XJFDIO"
      },
      "source": [
        "def with_bonus_smoothing(type_smoothing,validation):\n",
        "  \"\"\" \n",
        "      given a question -> word1 word2 xxxxx word3\n",
        "      predicting using probability of P(xxxxx|word2) * P(word3|xxxxx) using the future context\n",
        "      \n",
        "      probability calculation is same as the previous part\n",
        "      type_smoothing=1 is for no smoothing\n",
        "      type_smoothing=2 is for add 1 smoothing\n",
        "      type_smoothing=1 is for add k smoothing\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  if(type_smoothing==1):\n",
        "    file2.write(\"without smoothing \\n\")\n",
        "  elif(type_smoothing==2):\n",
        "    file2.write(\"add 1 smoothing \\n\")\n",
        "  else:\n",
        "    file2.write(\"add k smoothing \\n\")\n",
        "\n",
        "  correct=0\n",
        "  k=100    #for add k smoothing\n",
        "  for i in validation:\n",
        "    question=nltk.word_tokenize(i['question'].lower())\n",
        "    xxxxx_index=question.index('xxxxx')\n",
        "    before=''\n",
        "    after=''\n",
        "\n",
        "    if(xxxxx_index!=0):\n",
        "      before=question[xxxxx_index-1]\n",
        "    \n",
        "    if(xxxxx_index!=len(question)-1):\n",
        "      after=question[xxxxx_index+1]\n",
        "\n",
        "    prob=0\n",
        "    optimal_ans=''\n",
        "    \n",
        "    if(len(before)!=0 and len(after)!=0):\n",
        "      for j in i['options']:\n",
        "        j=j.lower()\n",
        "        to_check1=(before,j)\n",
        "        to_check2=(j,after)\n",
        "        \n",
        "        if(to_check1 not in bigram_count):\n",
        "            bigram_count[to_check1]=0\n",
        "\n",
        "        if(to_check2 not in bigram_count):\n",
        "            bigram_count[to_check2]=0\n",
        "\n",
        "        if(type_smoothing!=1):\n",
        "          if(before not in unigram_count):\n",
        "            unigram_count[before]=0\n",
        "            \n",
        "          if(j not in unigram_count):\n",
        "            unigram_count[before]=0\n",
        "\n",
        "        bc_tocheck1=bigram_count[to_check1]\n",
        "        bc_tocheck2=bigram_count[to_check2]\n",
        "        uc_before=unigram_count[before]\n",
        "        uc_j=unigram_count[j]\n",
        "        len_vocab=len(vocabulary)\n",
        "\n",
        "        if(type_smoothing==1 and (j in unigram_count and before in unigram_count)):\n",
        "          if((bc_tocheck1/uc_before)*(bc_tocheck2/uc_j)>prob):\n",
        "            prob=(bc_tocheck1/uc_before)*(bc_tocheck2/uc_j)\n",
        "            optimal_ans=j\n",
        "\n",
        "        elif(type_smoothing==2):\n",
        "          if(((bc_tocheck1+1)/(uc_before+len_vocab))*((bc_tocheck2+1)/(uc_j+len_vocab))>prob):\n",
        "            prob=((bc_tocheck1+1)/(uc_before+len_vocab))*((bc_tocheck2+1)/(uc_j+len_vocab))\n",
        "            optimal_ans=j\n",
        "\n",
        "        elif(type_smoothing==3):\n",
        "          if(((bc_tocheck1+1)/(uc_before+k*len_vocab))*((bc_tocheck2+1)/(uc_j+k*len_vocab))>prob):\n",
        "            prob=((bc_tocheck1+1)/(uc_before+k*len_vocab))*((bc_tocheck2+1)/(uc_j+k*len_vocab))\n",
        "            optimal_ans=j\n",
        "\n",
        "    if(optimal_ans==i['answer']):\n",
        "      correct+=1\n",
        "\n",
        "    output=\"correct ans : \"+i['answer'].lower()+\", prediction is: \"+optimal_ans + \"\\n\"\n",
        "    file2.write(output)\n",
        "\n",
        "  print(\"Accuracy with type\"+str(type_smoothing)+\" smoothing is \"+str(100*correct/len(validation))+\" %\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpH_fK9iJrXZ",
        "outputId": "ce43be5e-4110-4694-bb3b-fa6b37e5246f"
      },
      "source": [
        "with_bonus_smoothing(1,validation)\n",
        "with_bonus_smoothing(2,validation)\n",
        "with_bonus_smoothing(3,validation)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with type1 smoothing is 70.0 %\n",
            "Accuracy with type2 smoothing is 64.7 %\n",
            "Accuracy with type3 smoothing is 62.2 %\n"
          ]
        }
      ]
    }
  ]
}