{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP ASSIGNMENT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.casual import EMOTICON_RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('a01_spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6      ham  Even my brother is not like to speak with me. ...\n",
       "7      ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8     spam  WINNER!! As a valued network customer you have...\n",
       "9     spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1 (Words should not contain any punctuation, only a-zA-z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words starting with vowels are: 21496\n",
      "Words starting with consonants are: 64171\n"
     ]
    }
   ],
   "source": [
    "def task1(i):\n",
    "    regex1=r\"\\b[aeiouAEIOU]+[a-zA-Z]*\\b\"\n",
    "    regex2=r\"\\b[qwrtypsdfghjklzxcvbnmQWRTYPSDFGHJKLZXCVBNM]+[a-zA-Z]*\\b\"\n",
    "\n",
    "    tokenizer1=RegexpTokenizer(regex1)\n",
    "    tokenizer2=RegexpTokenizer(regex2)\n",
    "\n",
    "    words_consonant=0\n",
    "    words_vowel=0\n",
    "    \n",
    "    words_vowel+=len(tokenizer1.tokenize(i))\n",
    "    words_consonant+=len(tokenizer2.tokenize(i))\n",
    "\n",
    "    return(words_vowel,words_consonant)\n",
    "\n",
    "a=0\n",
    "b=0\n",
    "for i in data['Message']:\n",
    "    c,d=task1(i)\n",
    "    a+=c\n",
    "    b+=d\n",
    "    \n",
    "print(\"Words starting with vowels are: \"+str(a))\n",
    "print(\"Words starting with consonants are: \"+str(b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2 (Words should not contain any punctuation, only a-zA-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of capitalized words in spam category: 9.21968787515006\n",
      "Percentage of capitalized words in ham category: 5.529652449032995\n"
     ]
    }
   ],
   "source": [
    "# capitalized words definition used: full word should be of capital letters\n",
    "# word definition: should consist only of a-z or A-Z \n",
    "\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer as reg\n",
    "regex=r\"\\b[A-Z]+\\b\"\n",
    "token=reg(regex,gaps=False)\n",
    "\n",
    "spam_total=0\n",
    "spam_capital=0\n",
    "ham_total=0\n",
    "ham_capital=0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    tokenized=token.tokenize(data['Message'][i])\n",
    "    is_spam=False\n",
    "    is_ham=False\n",
    "    \n",
    "\n",
    "    if(data['Category'][i]=='ham'):\n",
    "        ham_capital+=len(tokenized)\n",
    "        ham_total+=len(nltk.word_tokenize(data['Message'][i]))\n",
    "        \n",
    "    else:\n",
    "        spam_capital+=len(tokenized)\n",
    "        spam_total+=len(nltk.word_tokenize(data['Message'][i]))\n",
    "    \n",
    "    \n",
    "print(\"Percentage of capitalized words in spam category: \"+str(float(spam_capital/spam_total)*100))\n",
    "print(\"Percentage of capitalized words in ham category: \"+str(float(ham_capital/ham_total)*100))           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3 (Email can contain a-zA-z0-9_, format is: string@string.string)\n",
    "## (Phone numbers should contain 10 to 12 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yijue@hotmail.com']\n",
      "['info@ringtoneking.co', 'tddnewsletter@emc1.co', 'info@txt82228.co', 'Dorothy@kiefer.com', 'ticket@kiosk.Valid', 'customersqueries@netvision.uk']\n",
      "['0125698789']\n",
      "['08002986030', '07732584351', '08000930705', '09061209465', '09066364589', '07742676969', '08719180248', '09064019788', '08712300220', '07046744435', '447801259231', '09058094597', '08000930705', '08002986906', '08708800282', '08000839402', '08717205546', '09057039994', '08000839402', '08000938767', '09064012103', '07123456789', '09111032124', '09058094455', '09066382422', '09061743806', '09061209465', '07815296484', '08718738001', '08002988890', '08712402050', '07753741225', '08715203677', '07946746291', '07880867867', '09064019014', '08715203694', '08718711108', '08452810071', '08712404000', '08000938767', '08714712388', '08714712394', '08706091795', '09066368327', '08000407165', '08701417012', '07821230901', '08002888812', '08000930705', '09066350750', '09061104283', '08719899217', '09058094565', '08712460324', '09063440451', '09061749602', '08718720201', '09050001808', '09063458130', '08000839402', '09065394514', '09058097218', '09058091854', '09050003091', '09061221061', '09061790121', '449050000301', '08000776320', '08450542832', '08719181503', '08712402779', '08718730555', '08712460324', '09096102316', '09701213186', '08707509020', '08718730666', '08712460324', '08000839402', '09099725823', '09066362231', '09066362231', '07801543489', '09099726395', '08707509020', '08718720201', '08719899230', '09061790121', '08701237397', '09058094599', '08712317606', '09071512433', '09099726429', '09050001295', '09063442151', '08718727870', '09094646899', '09041940223', '08081560665', '07786200117', '07734396839', '08712300220', '08001950382', '08718726970', '09058094594', '09061743811', '09090900040', '08452810073', '08715500022', '08702490080', '09066358152', '09090204448', '09050090044', '09065394973', '08000407165', '09061743810', '08006344447', '08000938767', '09094100151', '09066362220', '07742676969', '08719180248', '08717168528', '08001950382', '09066362231', '09050090044', '08714712379', '09065069120', '09099726481', '09065069154', '01223585334', '08000839402', '09056242159', '09061209465', '008704050406', '08718726971', '09066368753', '08000839402', '08712402902', '09058094565', '08702840625', '08718720201', '09066612661', '09063458130', '09056242159', '09066362206', '08000930705', '09050003091', '08000839402', '09066612661', '09066612661', '08715203028', '08718727870', '08002986906', '08712103738', '07123456789', '09058099801', '08707509020', '08707509020', '08000839402', '08718720201', '09058094454', '08714714011', '07821230901', '08000839402', '08707509020', '09058099801', '08000839402', '08712400603', '08708034412', '08452810073', '09094646899', '01223585334', '09053750005', '08707509020', '08081560665', '07786200117', '08712101358', '09066358361', '09065989180', '08718720201', '08718726978', '09058097189', '09061221066', '08712300220', '07090201529', '09066364349', '08081263000', '08712402972', '09058095201', '09066364589', '08000930705', '08002888812', '09066368470', '08448714184', '09099726553', '09061221066', '01223585236', '447797706009', '09050001808', '08000776320', '09071517866', '9061100010', '09050000460', '07734396839', '02085076972', '08000930705', '09066660100', '08701417012', '07090298926', '08718727870', '447797706009', '09058095107', '08717895698', '08712466669', '08000930705', '09061104276', '08000839402', '09061743806', '09066358152', '08719181259', '08000930705', '08712402050', '09061743386', '08001950382', '08000839402', '08000930705', '087104711148', '08712300220', '09066364311', '09065989182', '008704050406', '08709501522', '09064017305', '08717111821', '07973788240', '08715203649', '08000930705', '09058094599', '08707500020', '09061790125', '08712460324', '09050090044', '08000930705', '08002986030', '09066382422', '08000839402', '07008009200', '08712402578', '09050090044', '09058091854', '09061744553', '447801259231', '09058094597', '09058091870', '08000930705', '8000930705', '08718730666', '08712460324', '02073162414', '09061744553', '08712317606', '09071512432', '09066350750', '08712300220', '08717509990', '08000930705', '09066380611', '08718738002', '07099833605', '07808726822', '09061221066', '09061790121', '08708034412', '09061702893', '09077818151', '08719180219', '08715203652', '09061743386', '08718726270', '09064019014', '08000930705', '08712101358', '08006344447', '09058094507', '08712300220', '08714712412', '08718720201', '08715203685', '09094646631', '08712460324', '08000839402', '08001950382', '08701417012', '09066361921', '08715205273', '08002986906', '09050090044', '09063458130', '08452810073', '09050090044', '08000839402', '08715203656', '09066364311', '09064015307', '09061213237', '09061790126', '09061213237', '08712405020', '09095350301', '08712460324', '09061221066', '09066380611', '08717507382', '08718726270', '08715705022', '09058094583', '08002986906', '09050000332', '09064017295', '08448350055', '07808247860', '08719899229', '09061221061', '08718738001', '02073162414', '087187272008']\n",
      "\n",
      "Ham percentage for emails: 0.017946877243359655\n",
      "Spam percentage for emails: 0.10768126346015794\n",
      "\n",
      "Ham percentage for phone numbers: 0.017946877243359655\n",
      "Spam percentage for phone numbers: 5.886575735821967\n",
      "1\n",
      "328\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer as reg\n",
    "token=reg('\\s+',gaps=True)\n",
    "\n",
    "spam_emails=[]\n",
    "spam_numbers=[]\n",
    "ham_emails=[]\n",
    "ham_numbers=[]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    tokenized=token.tokenize(data['Message'][i])\n",
    "    for j in tokenized:\n",
    "        if re.findall(r\"\\w+@\\w+\\.\\w+\",j):\n",
    "            if(data['Category'][i]=='ham'):\n",
    "                for k in re.findall(r\"\\w+@\\w+.\\w+\",j):\n",
    "                    ham_emails.append(k)\n",
    "            else:\n",
    "                for k in re.findall(r\"\\w+@\\w+.\\w+\",j):\n",
    "                    spam_emails.append(k)\n",
    "                    \n",
    "        if re.findall(r\"[0-9]{10,12}$\",j):\n",
    "            if(data['Category'][i]=='ham'):\n",
    "                for k in re.findall(r\"\\b[0-9]{10,12}\\b\",j):\n",
    "                    ham_numbers.append(k)\n",
    "            else:\n",
    "                for k in re.findall(r\"\\b[0-9]{10,12}\\b\",j):\n",
    "                    spam_numbers.append(k)\n",
    "\n",
    "\n",
    "            \n",
    "print(ham_emails)\n",
    "print(spam_emails)\n",
    "print(ham_numbers)\n",
    "print(spam_numbers)\n",
    "print()\n",
    "print(\"Ham percentage for emails: \"+str(float(100*len(ham_emails)/(len(data['Category']=='ham')))))\n",
    "print(\"Spam percentage for emails: \"+str(float(100*len(spam_emails)/(len(data['Category']=='spam')))))\n",
    "print()\n",
    "print(\"Ham percentage for phone numbers: \"+str(float(100*len(ham_numbers)/(len(data['Category']=='ham')))))\n",
    "print(\"Spam percentage for phone numbers: \"+str(float(100*len(spam_numbers)/(len(data['Category']=='spam')))))\n",
    "\n",
    "print(len(ham_numbers))\n",
    "print(len(spam_numbers))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4 (Monetary values will be of the form: [Symbol][Number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$1', '$95', '$50', '£6', '£48,', '$50', '£50', '$700', '$900', '$700', '$900', '£33', '£6', '$2', '$140', '$180', '$900']\n",
      "['£1', '£900', '£100,000', '£5', '£1500', '£1000', '£1', '£1000', '£5000', '£1000', '£1000', '£2000', '$350', '£1', '£1000', '£900', '£1000', '£5000', '£1', '£200', '£1000', '£150', '£250', '£2000', '£500', '£75,000', '£500', '£2,000', '£150', '£3', '£1000', '£5000,', '£250', '£350', '£350', '£1', '£1000', '£2000', '£10', '£3', '£79', '£500', '£1', '£200', '£1', '£3', '£150', '£3', '£200', '£1000', '£500', '£100', '£150', '£3', '£150', '£3', '£1', '£500', '£100', '£1', '£1', '£1000', '£10,000', '£800', '£1000', '£5000', '£100', '£800', '£3', '£2000', '£100', '£2,000', '£500', '£1', '£3', '£500', '£100', '£1000', '£250', '£1', '£500', '£3', '£1', '£500', '£100', '£250', '£100', '£1', '£2,000', '£500', '£1', '£2,000', '£100', '£2000', '£350', '£250', '£1250', '£7', '£1', '£800', '£350', '£350', '£1', '£5000', '£2000', '£10', '£500', '£75,000', '£50', '£1000', '£2000', '£5000', '£5000', '£250', '£1', '£5000', '£1', '£200', '£1000', '£100', '£1000', '£2000', '£100', '£2,000', '£1', '£150', '£3', '£3', '£1000', '£2000', '£100', '£500', '£100', '£2,000', '£5000', '£1', '£1000', '£2000', '£4', '£1', '£4', '£250', '£500', '£100', '£100', '£1', '£2,000', '£2000', '£3', '£250', '£2000', '£1000', '£250', '£150', '£3', '£10,000', '£900', '£250', '£3', '£1', '£1', '£250', '£1', '£1', '£2,000', '£500', '£1', '£800', '£12', '£5000', '£1000', '£250', '£50', '£50', '£500', '£2,000', '£800', '£150', '£3', '£2000', '£1,500', '£100', '£800', '£500', '£75,000', '£100', '£1450', '£2000', '£200', '£1', '£2000', '£2,000', '£10,000', '£2000', '£1500', '£1500', '£1', '£800', '$5', '£1000', '£1', '£800', '£54', '£71', '£10', '£200', '£10', '£10', '£1000', '£1', '£1', '£200', '£1', '£1000', '£2000', '£1', '£900', '£500', '£100', '£3', '£250', '£1', '08717890890£', '£1', '£900', '£5000', '£3', '£150', '$5', '£400', '£1000', '£5000', '£350', '£350', '£1000', '£5000', '£1000', '£5000', '£100', '£1', '£1', '£38', '£1', '£5000', '£1', '£10,000', '£1', '£100', '£5', '£900', '£5000', '£1', '£600', '£4', '£1000', '£1', '£400', '£1,50', '£1', '£1000', '£7', '£250', '£1', '£1', '£1000', '£5000', '£350', '£400', '£400', '£2,000', '£2000', '£100', '£10,000', '£900', '£1000', '£1', '£150', '£350', '£350', '£500', '£1000', '£2000', '£2000', '£2', '£2', '£350', '£350', '£500', '£125', '£100', '£1000', '£2000', '£450', '£200', '£1', '£1000', '£200', '£5000', '£1', '£33', '£5000', '£1', '£350', '£5000', '£2000', '£100,000', '£5000', '£500', '£100', '£1', '£400', '£400', '£250', '£1000', '£33', '£1000', '£1000', '£4', '£3', '£250', '£800', '£750']\n",
      "\n",
      "Ham percentage for monetary values: 0.3050969131371141\n",
      "Spam percentage for monetary values: 5.778894472361809\n",
      "17\n",
      "322\n"
     ]
    }
   ],
   "source": [
    "spam_monetary=[]\n",
    "ham_monetary=[]\n",
    "\n",
    "reg_string=r'[$£][0-9,]+'\n",
    "reg_string1=r'[0-9,]+[$£]'\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if (re.findall(reg_string,data['Message'][i]) or re.findall(reg_string1,data['Message'][i])):\n",
    "        for j in re.findall(reg_string,data['Message'][i]):\n",
    "            if(data['Category'][i]=='ham'):\n",
    "                ham_monetary.append(j)\n",
    "            else:\n",
    "                spam_monetary.append(j)\n",
    "        \n",
    "        for j in re.findall(reg_string1,data['Message'][i]):\n",
    "            if(data['Category'][i]=='ham'):\n",
    "                ham_monetary.append(j)\n",
    "            else:\n",
    "                spam_monetary.append(j)\n",
    "\n",
    "print(ham_monetary)\n",
    "print(spam_monetary)\n",
    "print()\n",
    "print(\"Ham percentage for monetary values: \"+str(float(100*len(ham_monetary)/(len(data['Category']=='ham')))))\n",
    "print(\"Spam percentage for monetary values: \"+str(float(100*len(spam_monetary)/(len(data['Category']=='spam')))))\n",
    "print(len(ham_monetary))\n",
    "print(len(spam_monetary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5 (Emoticons using nltk.tokenize.casual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d:', 'p:', ':)', 'p;', ':)', ':)', ':)', 'P:', ':)', 'd:', ':)', ':)', ':)', ':)', ':)', ';D', ';D', ';D', ':)', ':)', ':)', 'd:', 'p;', 'p;', ':)', 'p;', 'd:', ':-)', 'p:', ':)', ':(', ':)', ':)', ':-)', ':-D', 'd8', ':)', ':)', ':-)', 'd8', ':-)', ':D', '=D', ':)', 'p;', 'd:', ';:', ':-)', ':)', ':)', ':)', ':)', ':)', 'd:', ':)', 'p:', '8/', ';)', ':)', ':)', 'p;', ':-D', '8/', ':)', 'p:', 'p;', 'p;', ':)', ':-)', ':-)', ';)', ';D', ':)', ':)', 'p;', 'd:', '=D', 'd:', ':)', 'd:', ':)', ':-)', ':)', ':)', 'p:', ':)', 'p;', ':)', ':)', ':)', ';D', 'P:', 'p:', ':)', ':-)', '8p', ':/', ':)', ':)', ':-)', ':)', ':)', ':)', 'p;', 'p;', ':)', 'p;', 'd:', 'd8', ':-)', ':-)', 'D=', ':)', ':-)', ';-)', '8d', ';D', 'p:', ':-)', ':)', ':)', ';-)', ':-D', ':D', ':)', ':-(', ':-(', ':/', 'p;', ':)', ';D', ':)', ':)', ':/', ':-)', ':-)', ':)', 'p;', ':-)', 'p;', 'p;', ':)', ':)', 'd8', 'P:', 'p;', ':-)', 'p;', ':)', ':)', ':-D', 'd:', ':8', ';D', ';D', 'd:', ':)', ':)', ':)', ':)', ':)', '8/', ':)', 'p;', ':)', ':-)', '::', '::', ':)', ':-)', ':-)', ';-)', ':)', ':)', ':-)', ':)', 'd:', 'p;', ':)', ':)', ':-)', 'd:', ':-(', ':-)', 'P:', ':-P', 'p;', ':-)', ':(', 'p;', 'p;', ':)', 'd:', ':)', 'd:', ':8', ':)', ':-)', ';-(', 'p;', ':)', ':)', ':)', ':)', 'd8', ':)', ':)', 'p;', 'p;', ':)', ':-(', ':)', ':)', ':-)', ':-)', ':)', ':)', ':(', 'p;', ':)', ':)', ':)', ':)', 'd:', 'p;', 'd:', ':)', ':-D', ':)', 'p:', ':-(', ':)', ';D', ':)', 'd;', 'p;', ':-)', ':-)', ':(', 'p:', ':-)', ';)', ':)', ':)', '8:', ':)', ':-)', ':)', ':)', ':)', ':)', ':)', ':-)', ':)', ';)', '=/', 'd8', ':-(', ':)', 'p;', ':-)', 'p:', 'd8', ':)', ':-)', ':D', ';)', ':-(', 'p;', ';D', 'd:', ':)', '8p', ':)', 'p;', ':-(', ':)', ':)', ':)', ':/', ';D', ':)', 'p;', 'p;', ':)', ':)', 'p;', 'p;', 'p;', 'p;', ':)', ':)', ':-)', ':-)', 'P:', ':)', ':)', ':)', ':)', ':)', 'P:', 'p;', ':)', ':-)', 'p;', ':)', ':-)', 'd:', ':)', ':)', ':-)', ':)', ';D', 'd:', ':)', ':-(', 'd8', 'p:', ':)', ':)', ':)', 'p;', 'p;', ':)', ':)', 'p;', 'p;', ':-(', 'p;', 'p;', 'p;', ':)', ':)', 'd8', ';D', ';D', ':-)', ':-)', 'p;', ':-(', ':-)', ':)', ';D', 'p;', 'd:', 'd8', ':)', ':)', 'd8', ':)', ':-)', 'p;', ':)', ':-)', ';-)', 'd:', 'd:', ':)', ':-)', 'p;', 'd:', 'D=', ':/', 'p:', '=D', ':-(', ':)', ':)', ':)', ':-)', ':)', ':)', ';-)', ':)', 'p:', ':-)', ':-)', ':)', ';D', ':)', ':-)', ';-)', ':-)', ':-(', ':)', 'p:', 'p;', ':)', 'd:', ';)', ':-P', ':)', 'd:', ':(', ';D', 'd:', ':)', ':)', ':)', 'd:', 'p;', 'd:', ':)', ':-)', ':-D', ';-)', 'p;', ':)', ':)', ':-)', ':-)', 'p;', 'p:', 'p;', 'd:', ':)', 'p:', ':-(', ':)', 'p;', 'p;', ':-(', ':-)', ':-)', ';D', 'p;', ':)', '8p', '::', '::', ':-)', ':)', ':-|', 'd:', '8p', 'd:', ':8', 'p;', 'p;', ';)', ':-)', ':)', ':)', '=D', ':)', ':-)', ':)', ':)', ':)', ':-)', 'p;', ':)', 'p;', 'p:', ':)', ':)', ':)', 'p;', 'p;', ':-)', ':)', ';D', ':)', 'd:', ':-)', 'p:', 'd=', '8:', ':)', 'p;', ':)', ':-)', ':-)', 'p:', 'd8', ':)', ':)', '=)', ':-)', 'd8', 'd8', 'p;', 'p;', 'p:', ':)', ':-)', ';-)', ':)', '8p', ':-)', ':-)', ';-)', 'p:', 'd8', 'p:', ':)', ':)', ':)', ':)', ':-P', 'p;', ':)', ':/', 'p:', ';D', ':)', ':)', 'p:', 'p;', ':-)', ':(', ':)', ':)', ':)', ':-)', ':(', ':)', 'p:', ':-)', ':-P', 'p;', ':-/', ':-)', ':)', ':)', ':)', ':-)', ':)', ':)', ';:', ':)', ':)', ';)', ';D', 'd:', 'p;', 'd:', ':)', 'd:', '8p', ':(', ';-(', ':-)', ':)', ':-)', ':)', ':)', ':-)', '=D', 'd:', ':-)', ':-)', ':)', ':-)', ':-)', ';-)', ':)', ':)', ':)', 'p;', ':)', ':)', 'p;', ':)', ';)', ':)', ':)', ':)', ':)', ':-)', ':-)', ':-)', ':-)', ':)', ':-)', ':)', ':-)', ':-)', ':-)', 'p;', 'p;', 'p;', 'p:', 'd:', ':D', ':)', ':)', ';D', ':)', ':)', ':)', 'p:', ':)', ':)', ':-)', ':-)', ':)', ':-)', ':-(', '8p', ':)', 'p;', ':-)', 'd8', 'd8', 'p;', ':)', 'p;', 'p;', ':-)', ':/', ':)', ':D', ';)', ':-)', ':)', ':)', ':-)', ':)', ':)', ':)', ':)', ':-)', 'd8', ':)', ':-)', 'p;', ':)', ':)', ':)', 'p;', ':-)', ':)', 'd:', 'd:', ':-(', ':-)', ':)', ':-)', ':)', ';-)', ':-)', 'p;', '8p', '8p', 'p;', ':)', 'p;', ':)', ':/', 'd=', ':)', ';:', ';:', ':-)', '8d', ':(', 'p;']\n",
      "Count of emoticons is: 658\n"
     ]
    }
   ],
   "source": [
    "all_emoticons=[]\n",
    "for i in range(len(data)):\n",
    "    token=EMOTICON_RE.findall(data['Message'][i])\n",
    "    for j in token:\n",
    "        all_emoticons.append(j)\n",
    "print(all_emoticons)\n",
    "print(\"Count of emoticons is: \"+str(len(all_emoticons)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"C's\", \"don't\", \"it's\", \"week's\", \"I'd\", \"I'm\", \"don't\", \"I've\", \"I've\", \"i'm\", \"I'm\", \"I'm\", \"mom's\", \"I'm\", \"we're\", \"I'll\", \"there's\", \"that's\", \"that's\", \"doesn't\", \"won't\", \"I'll\", \"roommate's\", \"How's\", \"you'd\", \"i'm\", \"Didn't\", \"can't\", \"don't\", \"didn't\", \"I'm\", \"he's\", \"I'm\", \"didn't\", \"don't\", \"I'm\", \"I'm\", \"I'll\", \"can't\", \"Ken's\", \"Ta's\", \"I'll\", \"don't\", \"I'm\", \"I'm\", \"I'm\", \"haven't\", \"don't\", \"I'm\", \"You're\", \"didn't\", \"I'll\", \"isn't\", \"he's\", \"I'm\", \"i'm\", \"i'm\", \"B'day\", \"he's\", \"hasn't\", \"shouldn't\", \"I'm\", \"UK's\", \"It's\", \"You'll\", \"you're\", \"haven't\", \"I'm\", \"i'm\", \"I'm\", \"see's\", \"I'm\", \"I'm\", \"one's\", \"i'm\", \"you've\", \"won't\", \"We'd\", \"Where's\", \"I'm\", \"I've\", \"you've\", \"B'day\", \"What's\", \"haven't\", \"there's\", \"I'll\", \"I'm\", \"I'm\", \"i'm\", \"it's\", \"u've\", \"i'm\", \"don't\", \"didn't\", \"I'm\", \"you're\", \"Don't\", \"I'm\", \"i'm\", \"we're\", \"I'll\", \"guy's\", \"I'll\", \"C's\", \"it's\", \"Can't\", \"it's\", \"won't\", \"aren't\", \"he'll\", \"friend's\", \"he's\", \"i'm\", \"We'll\", \"I'd\", \"that's\", \"you're\", \"I'm\", \"employer's\", \"I'll\", \"that's\", \"can't\", \"I've\", \"you're\", \"I'm\", \"I'm\", \"I'm\", \"i'm\", \"you're\", \"audrey's\", \"you're\", \"I'm\", \"did't\", \"there's\", \"that's\", \"there's\", \"i'm\", \"shouldn't\", \"We'll\", \"you've\", \"i've\", \"that's\", \"can't\", \"I'll\", \"I'll\", \"i'm\", \"She'll\", \"I'm\", \"I've\", \"I'll\", \"I'm\", \"where's\", \"that's\", \"C's\", \"I'll\", \"don't\", \"I'll\", \"UK's\", \"don't\", \"I'm\", \"you're\", \"When're\", \"i've\", \"account's\", \"I'll\", \"it's\", \"dat's\", \"how's\", \"I've\", \"What's\", \"g's\", \"i'm\", \"i'm\", \"that's\", \"that's\", \"That's\", \"won't\", \"x'mas\", \"Today's\", \"you're\", \"I'll\", \"I've\", \"doesn't\", \"doesn't\", \"bloke's\", \"you're\", \"I've\", \"it's\", \"you'll\", \"i'm\", \"it's\", \"Can't\", \"I'm\", \"i'll\", \"how's\", \"i'll\", \"I'm\", \"T's\", \"C's\", \"I'm\", \"mum's\", \"it's\", \"I'm\", \"it's\", \"It's\", \"YOU'RE\", \"it's\", \"that's\", \"I'm\", \"I'll\", \"I'm\", \"can't\", \"isn't\", \"I'm\", \"dealer's\", \"mum's\", \"Carlos'll\", \"won't\", \"i'll\", \"I've\", \"u're\", \"I'm\", \"I'm\", \"Can't\", \"I'm\", \"It's\", \"I'm\", \"I'm\", \"alex's\", \"I'll\", \"sms'd\", \"don't\", \"I'm\", \"S'fine\", \"i'm\", \"didn't\", \"won't\", \"u're\", \"hadn't\", \"we've\", \"we'll\", \"don't\", \"phone's\", \"I'm\", \"I'll\", \"They're\", \"can't\", \"don't\", \"you're\", \"couldn't\", \"she's\", \"i've\", \"isn't\", \"It's\", \"it's\", \"C's\", \"don't\", \"I'll\", \"ain't\", \"don't\", \"It's\", \"there's\", \"It's\", \"Where's\", \"doesn't\", \"he'll\", \"How's\", \"Moon's\", \"We've\", \"that's\", \"month's\", \"i'm\", \"Prashanthettan's\", \"jay's\", \"don't\", \"don't\", \"can't\", \"there's\", \"Today's\", \"I've\", \"i've\", \"he's\", \"I'll\", \"don't\", \"I'll\", \"I'm\", \"She's\", \"It's\", \"u're\", \"i'm\", \"Doesn't\", \"shouldn't\", \"I'm\", \"don't\", \"i've\", \"we'll\", \"ain't\", \"We're\", \"what's\", \"Zaher's\", \"Can't\", \"I'm\", \"didn't\", \"I'm\", \"ashley's\", \"did't\", \"I'm\", \"I'm\", \"I'm\", \"won't\", \"term's\", \"you'll\", \"we'll\", \"I've\", \"you're\", \"dobby's\", \"u'll\", \"isn't\", \"it's\", \"i've\", \"can't\", \"he'll\", \"I'm\", \"can't\", \"can't\", \"don't\", \"It's\", \"i'm\", \"don't\", \"can't\", \"they're\", \"Hasn't\", \"It's\", \"I'll\", \"we'll\", \"You're\", \"I'm\", \"I'll\", \"mom's\", \"I'm\", \"it's\", \"Jay's\", \"I'm\", \"they're\", \"how're\", \"We're\", \"I'm\", \"That's\", \"I'll\", \"I'll\", \"U've\", \"don't\", \"FR'NDSHIP\", \"don't\", \"cali's\", \"Don't\", \"I'll\", \"u'll\", \"we're\", \"I've\", \"you'll\", \"I'm\", \"I'm\", \"don't\", \"I'll\", \"B'day\", \"it's\", \"don't\", \"don't\", \"let's\", \"shade's\", \"she's\", \"I'm\", \"i'll\", \"I'll\", \"didn't\", \"He's\", \"There's\", \"There's\", \"C's\", \"I'm\", \"I'm\", \"it's\", \"I'm\", \"you'll\", \"I'm\", \"isn't\", \"There's\", \"he's\", \"I'll\", \"We're\", \"It's\", \"I've\", \"ain't\", \"I'll\", \"won't\", \"I'll\", \"i'm\", \"U'll\", \"I'll\", \"I'm\", \"I'm\", \"I'm\", \"don't\", \"i'm\", \"I'm\", \"I'm\", \"We've\", \"C's\", \"I've\", \"doesn't\", \"i'm\", \"what's\", \"I'll\", \"Don't\", \"i'm\", \"she's\", \"I'll\", \"jay's\", \"I'll\", \"It's\", \"patty's\", \"What's\", \"I'll\", \"he's\", \"he's\", \"leona's\", \"i'll\", \"I'm\", \"George's\", \"i'm\", \"I'm\", \"don't\", \"that's\", \"I'm\", \"We're\", \"It's\", \"That's\", \"you'd\", \"doesn't\", \"I'll\", \"I've\", \"U've\", \"She's\", \"I'll\", \"I'll\", \"Don't\", \"don't\", \"i'm\", \"haven't\", \"how's\", \"That's\", \"i'll\", \"i'm\", \"blake's\", \"I'm\", \"I'm\", \"it's\", \"i'm\", \"I'm\", \"I'll\", \"don't\", \"can't\", \"I'm\", \"I'm\", \"someone's\", \"I'm\", \"don't\", \"won't\", \"you'll\", \"didn't\", \"i've\", \"don't\", \"i'll\", \"How's\", \"You're\", \"I'm\", \"don't\", \"you're\", \"month's\", \"don't\", \"That's\", \"i'm\", \"I'm\", \"It's\", \"that's\", \"derek's\", \"It's\", \"I'll\", \"I'll\", \"Let's\", \"That's\", \"I'll\", \"I'm\", \"I'll\", \"i'll\", \"we'll\", \"I'll\", \"how's\", \"we'll\", \"don't\", \"don't\", \"i'm\", \"I'd\", \"wasn't\", \"didn't\", \"we're\", \"We're\", \"they're\", \"shouldn't\", \"don't\", \"don't\", \"T's\", \"C's\", \"i'm\", \"There's\", \"I'm\", \"where's\", \"aren't\", \"It's\", \"that's\", \"It's\", \"don't\", \"I'm\", \"i'm\", \"can't\", \"don't\", \"How's\", \"Doesn't\", \"haven't\", \"UK's\", \"don't\", \"I'll\", \"we're\", \"finn's\", \"i'm\", \"won't\", \"Joy's\", \"Joy's\", \"It's\", \"it's\", \"I'm\", \"I'm\", \"I'll\", \"I'm\", \"blake's\", \"he's\", \"I'm\", \"I'll\", \"How's\", \"dat's\", \"I'll\", \"something's\", \"couldn't\", \"you're\", \"What's\", \"you'ld\", \"That's\", \"how's\", \"I'm\", \"won't\", \"B'tooth\", \"did't\", \"What's\", \"we'll\", \"didn't\", \"You'd\", \"wasn't\", \"don't\", \"wasn't\", \"don't\", \"don't\", \"can't\", \"i'm\", \"there's\", \"that's\", \"I'll\", \"knee's\", \"Don't\", \"I'll\", \"I'm\", \"Don't\", \"can't\", \"can't\", \"U've\", \"station's\", \"don't\", \"I'll\", \"C's\", \"There're\", \"I'm\", \"I've\", \"can't\", \"I'm\", \"i'll\", \"I'll\", \"It's\", \"don't\", \"don't\", \"idea's\", \"you're\", \"i'm\", \"There's\", \"I'll\", \"i've\", \"I'll\", \"you're\", \"I'm\", \"What's\", \"don't\", \"it's\", \"don't\", \"we'll\", \"I'm\", \"I'm\", \"I'll\", \"I'll\", \"i'm\", \"i'm\", \"I'll\", \"basket's\", \"I'm\", \"DIDN'T\", \"where's\", \"i'll\", \"Can't\", \"can't\", \"don't\", \"that's\", \"you'd\", \"I've\", \"don't\", \"it's\", \"I'm\", \"You're\", \"don't\", \"anybody's\", \"haven't\", \"can't\", \"i'm\", \"I'll\", \"I'm\", \"didn't\", \"i'm\", \"i'm\", \"Who's\", \"isn't\", \"I'm\", \"Wat's\", \"I'm\", \"i'm\", \"can't\", \"you've\", \"can't\", \"can't\", \"can't\", \"can't\", \"i'm\", \"didn't\", \"i'll\", \"I'll\", \"it's\", \"I'm\", \"I'm\", \"you're\", \"you've\", \"George's\", \"don't\", \"I'll\", \"wasn't\", \"It's\", \"I'm\", \"I'm\", \"He's\", \"I'm\", \"don't\", \"table's\", \"I'm\", \"b'day\", \"did'nt\", \"B'day\", \"everybody's\", \"I'm\", \"wat's\", \"I'm\", \"don't\", \"I'm\", \"i'm\", \"I'm\", \"i'm\", \"weren't\", \"i'll\", \"i'm\", \"i'm\", \"i'm\", \"God's\", \"there's\", \"no's\", \"What's\", \"I'm\", \"bb's\", \"doesn't\", \"I'll\", \"I'm\", \"it's\", \"I'm\", \"She's\", \"didn't\", \"don't\", \"b'day\", \"did'nt\", \"I've\", \"all's\", \"roommate's\", \"I'll\", \"he's\", \"don't\", \"that's\", \"that's\", \"that's\", \"that'll\", \"don't\", \"That's\", \"That's\", \"couldn't\", \"i'll\", \"I'll\", \"I'm\", \"you've\", \"it's\", \"I'm\", \"that's\", \"month's\", \"I'll\", \"where's\", \"How's\", \"I'm\", \"I'm\", \"don't\", \"don't\", \"I'm\", \"you'd\", \"I'm\", \"I'm\", \"Who's\", \"that's\", \"don't\", \"don't\", \"let's\", \"I'm\", \"haven't\", \"I'll\", \"anjola's\", \"I'm\", \"That's\", \"T's\", \"C's\", \"I'm\", \"hasn't\", \"he's\", \"doesn't\", \"can't\", \"I'll\", \"I've\", \"I'll\", \"I'm\", \"don't\", \"can't\", \"I'm\", \"I'm\", \"i'm\", \"i'd\", \"You've\", \"you'll\", \"that's\", \"don't\", \"I'm\", \"couldn't\", \"It's\", \"I'll\", \"i've\", \"I'm\", \"it's\", \"can't\", \"I'm\", \"i'm\", \"don't\", \"i'm\", \"I'll\", \"Joy's\", \"Joy's\", \"i've\", \"I'll\", \"I'll\", \"he'll\", \"There's\", \"What's\", \"we'll\", \"That's\", \"how's\", \"i've\", \"I'm\", \"I'm\", \"I'll\", \"I'll\", \"it's\", \"I'll\", \"i'm\", \"I'm\", \"didn't\", \"i'm\", \"don't\", \"you're\", \"UK's\", \"didn't\", \"It's\", \"he's\", \"he's\", \"i'm\", \"i'll\", \"didn't\", \"you're\", \"It's\", \"can't\", \"wasn't\", \"I'm\", \"I'm\", \"dramastorm's\", \"I'm\", \"I'm\", \"That's\", \"don't\", \"We'd\", \"I'll\", \"don't\", \"taylor's\", \"don't\", \"When's\", \"can't\", \"UK's\", \"I'm\", \"That's\", \"I'll\", \"I'm\", \"I'm\", \"i'm\", \"Who's\", \"I'm\", \"I'm\", \"What's\", \"I've\", \"i'm\", \"u'll\", \"didn't\", \"haven't\", \"I'll\", \"I'm\", \"i'm\", \"I'm\", \"It's\", \"we'll\", \"don't\", \"don't\", \"let's\", \"today's\", \"you're\", \"T's\", \"C's\", \"I'm\", \"today's\", \"I'll\", \"there's\", \"I'll\", \"I've\", \"i've\", \"it's\", \"you're\", \"you're\", \"what's\", \"Don't\", \"who's\", \"It's\", \"I'll\", \"You've\", \"I'm\", \"how's\", \"C's\", \"b'day\", \"did'nt\", \"B'day\", \"She's\", \"I'm\", \"i'm\", \"fren's\", \"I'm\", \"haven't\", \"I'm\", \"i've\", \"you're\", \"i'm\", \"i'm\", \"Harish's\", \"I'll\", \"didn't\", \"won't\", \"ron's\", \"it's\", \"there's\", \"I'll\", \"it's\", \"I'll\", \"what's\", \"I'll\", \"didn't\", \"i'm\", \"he's\", \"I've\", \"god's\", \"Wat's\", \"don't\", \"I'm\", \"tm'ing\", \"i've\", \"didn't\", \"doesn't\", \"don't\", \"don't\", \"i'll\", \"there's\", \"I'm\", \"I'm\", \"you're\", \"y'day\", \"I'll\", \"Don't\", \"i'm\", \"I'd\", \"I'm\", \"can't\", \"isn't\", \"it's\", \"did't\", \"i'm\", \"we'll\", \"we're\", \"I'll\", \"riley's\", \"You've\", \"how've\", \"Don't\", \"l'm\", \"your's\", \"didn't\", \"don't\", \"don't\", \"what's\", \"Don't\", \"i've\", \"i'll\", \"i'm\", \"how's\", \"don't\", \"didn't\", \"It's\", \"I'm\", \"did't\", \"haven't\", \"Don't\", \"how's\", \"That's\", \"it's\", \"haven't\", \"we're\", \"C's\", \"you've\", \"I've\", \"I'm\", \"You're\", \"you're\", \"I'll\", \"UK's\", \"he's\", \"Isn't\", \"can't\", \"I'm\", \"I'm\", \"Haven't\", \"I'll\", \"it's\", \"didn't\", \"WOULDN'T\", \"She's\", \"I've\", \"i've\", \"didn't\", \"What's\", \"I'll\", \"It's\", \"i'm\", \"it's\", \"you're\", \"I'll\", \"i've\", \"can't\", \"didn't\", \"I'm\", \"i'm\", \"she'll\", \"i'm\", \"I'm\", \"Don't\", \"I'm\", \"Who's\", \"it's\", \"I've\", \"I'm\", \"you'll\", \"you'll\", \"I'm\", \"It's\", \"don't\", \"it's\", \"don't\", \"I'm\", \"can't\", \"I'm\", \"It'll\", \"I'll\", \"Don't\", \"i'm\", \"I'm\", \"tt's\", \"I'm\", \"we're\", \"yetty's\", \"it's\", \"That's\", \"we're\", \"friend's\", \"haven't\", \"U're\", \"I'll\", \"she'll\", \"I'm\", \"they'll\", \"don't\", \"don't\", \"let's\", \"I'm\", \"I'm\", \"I'm\", \"I'll\", \"today's\", \"i've\", \"don't\", \"it's\", \"don't\", \"mum's\", \"I'm\", \"wouldn't\", \"how's\", \"Joy's\", \"Joy's\", \"won't\", \"don't\", \"don't\", \"i'd\", \"You've\", \"t's\", \"c's\", \"don't\", \"priscilla's\", \"how's\", \"everybody's\", \"there's\", \"Wherre's\", \"I'll\", \"I'm\", \"cann't\", \"i'll\", \"doesn't\", \"it's\", \"can't\", \"I'm\", \"I'm\", \"I'm\", \"that'd\", \"They're\", \"I'll\", \"I'm\", \"I'll\", \"I'm\", \"who's\", \"You'll\", \"don't\", \"I'm\", \"NY's\", \"Shahjahan's\", \"Mumtaz's\", \"Mumtaz's\", \"VALENTINE'S\", \"She's\", \"won't\", \"it's\", \"It's\", \"I'll\", \"how's\", \"I've\", \"she'll\", \"I'll\", \"YOU'VE\", \"You're\", \"we'll\", \"I'm\", \"Joy's\", \"Joy's\", \"I'll\", \"Moon's\", \"i'll\", \"don't\", \"doesn't\", \"I'll\", \"It's\", \"It's\", \"don't\", \"Wat's\", \"He's\", \"It's\", \"didn't\", \"we're\", \"you're\", \"What's\", \"I'm\", \"I'll\", \"I'm\", \"won't\", \"Can't\", \"I've\", \"I'll\", \"er'ything\", \"i'll\", \"it's\", \"I'm\", \"I'm\", \"i'm\", \"I'm\", \"Isn't\", \"hw'd\", \"wat'll\", \"espe'll\", \"i'm\", \"i'm\", \"doesn't\", \"you're\", \"I'm\", \"I'm\", \"i'd\", \"We'll\", \"that's\", \"didn't\", \"We'd\", \"don't\", \"Don't\", \"weekend's\", \"don't\", \"u'll\", \"It's\", \"armand's\", \"i'm\", \"can't\", \"i'm\", \"I'm\", \"I'm\", \"mom's\", \"I'll\", \"Let's\", \"I'm\", \"That'll\", \"It's\", \"that's\", \"anything's\", \"i'm\", \"there's\", \"I'm\", \"I've\", \"I'm\", \"we're\", \"guy's\", \"there're\", \"Gumby's\", \"we're\", \"I'm\", \"You've\", \"it's\", \"I'm\", \"She's\", \"I'm\", \"don't\", \"haven't\", \"wasn't\", \"I'm\", \"i'll\", \"I'm\", \"She's\", \"there's\", \"Can't\", \"won't\", \"I'm\", \"fakeye's\", \"I'm\", \"I'm\", \"That's\", \"you're\", \"I'll\", \"We'll\", \"He's\", \"won't\", \"there's\", \"I'm\", \"I'm\", \"I'm\", \"you're\", \"i'll\", \"can't\", \"week's\", \"that's\", \"can't\", \"U've\", \"Wat's\", \"didn't\", \"I'll\", \"i'm\", \"I'm\", \"I'm\", \"that's\", \"What's\", \"didn't\", \"you're\", \"we're\", \"I've\", \"blake's\", \"I'm\", \"I'm\", \"How's\", \"he's\", \"I'm\", \"I've\", \"i'll\", \"you're\", \"isn't\", \"ain't\", \"I'm\", \"uk's\", \"I'm\", \"I'll\", \"I'll\", \"Doesn't\", \"Don't\", \"Don't\", \"they're\", \"I'm\", \"aren't\", \"I'm\", \"I'll\", \"i'm\", \"you're\", \"can't\", \"Today's\", \"I'm\", \"That's\", \"i'm\", \"T's\", \"C's\", \"Haven't\", \"What's\", \"How's\", \"I'm\", \"I'm\", \"I'll\", \"Haven't\", \"he's\", \"I'm\", \"can't\", \"what's\", \"joke's\", \"I'll\", \"i've\", \"How's\", \"He's\", \"That's\", \"I'll\", \"won't\", \"I'm\", \"i'll\", \"I'll\", \"i'm\", \"it's\", \"I'm\", \"THERE'S\", \"I'm\", \"i'm\", \"mine's\", \"I'm\", \"Don't\", \"ugo's\", \"I'm\", \"friend's\", \"you're\", \"She's\", \"she's\", \"haven't\", \"PARTNER'S\", \"I'd\", \"THERE'S\", \"b'day\", \"did'nt\", \"You'll\", \"nobody's\", \"can't\", \"she's\", \"aunty's\", \"I'm\", \"I've\", \"don't\", \"don't\", \"That's\", \"don't\", \"u're\", \"i'm\", \"she'll\", \"I'm\", \"wasn't\", \"i've\", \"can't\", \"cann't\", \"he's\", \"I'll\", \"i'll\", \"I'm\", \"wouldn't\", \"didn't\", \"don't\", \"THERE'S\", \"can't\", \"I've\", \"Don't\", \"can't\", \"dsn't\", \"dsn't\", \"dsn't\", \"don't\", \"i'd\", \"that's\", \"you're\", \"don't\", \"don't\", \"haven't\", \"i'm\", \"we're\", \"I'm\", \"I'll\", \"I'll\", \"b'day\", \"did'nt\", \"I'll\", \"I'm\", \"he's\", \"wudn't\", \"didn't\", \"Can't\", \"it's\", \"I'ma\", \"i'll\", \"Don't\", \"don't\", \"I've\", \"I'm\", \"It's\", \"don't\", \"I'm\", \"it's\", \"don't\", \"I'm\", \"I've\", \"I'm\", \"It's\", \"that's\", \"shit's\", \"he's\", \"I'm\", \"I'll\", \"i'm\", \"can't\", \"don't\", \"she's\", \"we've\", \"havn't\", \"I'm\", \"you're\", \"wat's\", \"i'm\", \"aren't\", \"haven't\", \"I've\", \"hubby's\", \"Don't\", \"Party's\", \"I've\", \"I've\", \"won't\", \"I'm\", \"What's\", \"i'm\", \"I'm\", \"didn't\", \"T's\", \"C's\", \"I'll\", \"I'll\", \"It's\", \"I'll\", \"I'm\", \"don't\", \"won't\", \"I'm\", \"couldn't\", \"I'm\", \"I'm\", \"I'll\", \"is'LOVE\", \"I'm\", \"I'm\", \"won't\", \"god's\", \"don't\", \"I'll\", \"I'm\", \"There's\", \"I'm\", \"I'm\", \"That's\", \"he'll\", \"you're\", \"we'll\", \"Didn't\", \"How's\", \"i'll\", \"I'll\", \"I'm\", \"it's\", \"it's\", \"I'm\", \"There'll\", \"it's\", \"I've\", \"xin's\", \"it's\", \"I'm\", \"you're\", \"i'm\", \"Don't\", \"he's\", \"I'm\", \"It's\", \"I'm\", \"Wat's\", \"we're\", \"we'll\", \"We're\", \"i'm\", \"can't\", \"i'm\", \"don't\", \"we're\", \"I'm\", \"What's\", \"Don't\", \"I'll\", \"i'm\", \"I'm\", \"How's\", \"it's\", \"i'll\", \"can't\", \"That's\", \"I'm\", \"you'd\", \"I'm\", \"Wasn't\", \"didn't\", \"Dip's\", \"world's\", \"tyler's\", \"I'm\", \"can't\", \"don't\", \"didn't\", \"fuck's\", \"she's\", \"didn't\", \"i've\", \"wasn't\", \"wasn't\", \"didn't\", \"wouldn't\", \"I'm\", \"You've\", \"It'snot\", \"child's\", \"they're\", \"I'm\", \"i've\", \"didn't\", \"don't\", \"don't\", \"How's\", \"Where's\", \"how's\", \"I'm\", \"don't\", \"don't\", \"I'm\", \"I'm\", \"how's\", \"I'm\", \"didn't\", \"i'm\", \"I'm\", \"UNICEF's\", \"I'm\", \"I'm\", \"That's\", \"I'm\", \"I'm\", \"wasn't\", \"I'm\", \"There's\", \"olayiwola's\", \"I'm\", \"I'm\", \"U've\", \"It's\", \"you're\", \"she's\", \"I'd\", \"won't\", \"it's\", \"It's\", \"didn't\", \"i'm\", \"I'm\", \"there's\", \"I'll\", \"dad's\", \"cann't\", \"Moon's\", \"i'm\", \"didn't\", \"b'coz\", \"you've\", \"virgil's\", \"i'm\", \"you're\", \"I've\", \"I'll\", \"You'll\", \"it's\", \"I'm\", \"I'm\", \"I'll\", \"I'm\", \"i'm\", \"i'm\", \"Haven't\", \"sir's\", \"We'll\", \"it's\", \"i'm\", \"I'm\", \"i'll\", \"we'll\", \"what's\", \"i'm\", \"hasn't\", \"can't\", \"I'm\", \"i'll\", \"C's\", \"I'll\", \"It's\", \"I'm\", \"can't\", \"you'd\", \"It's\", \"i'm\", \"i'm\", \"he's\", \"I'm\", \"i'm\", \"how's\", \"Don't\", \"Haven't\", \"I'm\", \"can't\", \"I'm\", \"she's\", \"you'll\", \"It's\", \"don't\", \"it's\", \"It's\", \"I'm\", \"Don't\", \"u'll\", \"u'll\", \"he's\", \"When's\", \"can't\", \"haven't\", \"I'm\", \"that's\", \"i'm\", \"I'm\", \"i'm\", \"I'll\", \"How's\", \"i'm\", \"It's\", \"I'm\", \"shouldn't\", \"You're\", \"don't\", \"You'd\", \"wouldn't\", \"That's\", \"I'm\", \"it's\", \"he's\", \"I'll\", \"I'm\", \"ryan's\", \"You're\", \"I'm\", \"I'm\", \"we'd\", \"didn't\", \"I'll\", \"don't\", \"i'm\", \"isn't\", \"weather's\", \"I'm\", \"Don't\", \"i'm\", \"That's\", \"he's\", \"it's\", \"I'm\", \"u're\", \"when's\", \"there's\", \"don't\", \"How's\", \"I'm\", \"I'm\", \"mom's\", \"can't\", \"today's\", \"I'll\", \"it's\", \"you're\", \"wasn't\", \"You're\", \"I'm\", \"biola's\", \"i've\", \"i'd\", \"i'm\", \"that's\", \"I'll\", \"don't\", \"weekend's\", \"Where's\", \"mummy's\", \"who's\", \"T's\", \"C's\", \"i'll\", \"God's\", \"God's\", \"God's\", \"God's\", \"I've\", \"i'm\", \"mei's\", \"i've\", \"tantrum's\", \"I'll\", \"can't\", \"it's\", \"I'm\", \"I'm\", \"you're\", \"I'll\", \"priscilla's\", \"That's\", \"I've\", \"that's\", \"you'll\", \"I'll\", \"aren't\", \"don't\", \"i'd\"]\n",
      "Count of clitics: 1714\n"
     ]
    }
   ],
   "source": [
    "clitics=[]\n",
    "regex=r\"\\b[a-zA-Z]+'[a-zA-Z]+\\b\"\n",
    "tokenizer=RegexpTokenizer(regex)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    tokenized=tokenizer.tokenize(data['Message'][i])\n",
    "    for j in tokenized:\n",
    "        clitics.append(j)\n",
    "        \n",
    "print(clitics)\n",
    "print(\"Count of clitics: \"+str(len(clitics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!\n",
      "Hello, my love. What are you doing? Did you get to that interview today? Are you you happy? Are you being a good boy? Do you think of me?Are you missing me ?\n",
      "Hello handsome ! Are you finding that job ? Not being lazy ? Working towards getting back that net for mummy ? Where's my boytoy now ? Does he miss me ?\n",
      "Hello darlin ive finished college now so txt me when u finish if u can love Kate xxx\n",
      "Hello! Just got here, st andrews-boy its a long way! Its cold. I will keep you posted\n",
      "Hello my boytoy ... Geeee I miss you already and I just woke up. I wish you were here in bed with me, cuddling me. I love you ...\n",
      "Hello! Good week? Fancy a drink or something later?\n",
      "Hello. We need some posh birds and chaps to user trial prods for champneys. Can i put you down? I need your address and dob asap. Ta r\n",
      "Hello baby, did you get back to your mom's ? Are you setting up the computer now ? Filling your belly ? How goes it loverboy ? I miss you already ... *sighs*\n",
      "Hello which the site to download songs its urgent pls\n",
      "Hello lover! How goes that new job? Are you there now? Are you happy? Do you think of me? I wake, my slave and send you a teasing kiss from across the sea\n",
      "Hello from Orange. For 1 month's free access to games, news and sport, plus 10 free texts and 20 photo messages, reply YES. Terms apply: www.orange.co.uk/ow\n",
      "Hello my little party animal! I just thought I'd buzz you as you were with your friends ...*grins*... Reminding you were loved and send a naughty adoring kiss\n",
      "Hello beautiful r u ok? I've kinda ad a row wiv and he walked out the pub?? I wanted a night wiv u Miss u\n",
      "Hello. They are going to the village pub at 8 so either come here or there accordingly. Ok?\n",
      "Hello! How r u? Im bored. Inever thought id get bored with the tv but I am. Tell me something exciting has happened there? Anything! =/\n",
      "Hello. Damn this christmas thing. I think i have decided to keep this mp3 that doesnt work.\n",
      "Hello, my love ! How went your day ? Are you alright ? I think of you, my sweet and send a jolt to your heart to remind you ... I LOVE YOU! Can you hear it ? I screamed it across the sea for all the world to hear. Ahmad al Hallaq is loved ! and owned ! *possessive passionate kiss*\n",
      "Hello, my boytoy! I made it home and my constant thought is of you, my love. I hope your having a nice visit but I can't wait till you come home to me ...*kiss*\n",
      "HELLO U.CALL WEN U FINISH WRK.I FANCY MEETIN UP WIV U ALL TONITE AS I NEED A BREAK FROM DABOOKS. DID 4 HRS LAST NITE+2 TODAY OF WRK!\n",
      "Hello madam how are you ?\n",
      "Hello darling how are you today? I would love to have a chat, why dont you tell me what you look like and what you are in to sexy?\n",
      "Hello. No news on job, they are making me wait a fifth week! Yeah im up for some woozles and weasels... In exeter still, but be home about 3.\n",
      "Hello, yeah i've just got out of the bath and need to do my hair so i'll come up when i'm done, yeah?\n",
      "Hello hun how ru? Its here by the way. Im good. Been on 2 dates with that guy i met in walkabout so far. We have to meet up soon. Hows everyone else?\n",
      "Hello, my love! How goes that day ? I wish your well and fine babe and hope that you find some job prospects. I miss you, boytoy ... *a teasing kiss*\n",
      "Hello, hello, hi lou sorry it took so long 2 reply- I left mobile at friends in Lancaster, just got it bak Neway im sorry I couldnt make ur bday 2 hun!\n",
      "Hello, As per request from  &lt;#&gt;  Rs.5 has been transfered to you\n",
      "Hello. Sort of out in town already. That . So dont rush home, I am eating nachos. Will let you know eta.\n",
      "HELLO PEACH! MY CAKE TASTS LUSH!\n",
      "Hello boytoy ! Geeee ... I'm missing you today. I like to send you a tm and remind you I'm thinking of you ... And you are loved ... *loving kiss*\n",
      "\n",
      "Number of messages: 31\n"
     ]
    }
   ],
   "source": [
    "word=\"HeLLo\"\n",
    "number=0\n",
    "for i in range(len(data)):\n",
    "    tokenize=word_tokenize(data['Message'][i].lower())\n",
    "    if(len(tokenize)>0 and word.lower()==tokenize[0]):\n",
    "        print(data['Message'][i])\n",
    "        number+=1\n",
    "    \n",
    "print()\n",
    "print(\"Number of messages: \"+str(number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry, i'll call later ok bye\n",
      "off to lecture, cheery bye bye.\n",
      "Number of sentences ending with the custom word are: 2\n"
     ]
    }
   ],
   "source": [
    "word=\"bye\"\n",
    "count=0\n",
    "for i in range(len(data)):\n",
    "    token=sent_tokenize(data['Message'][i].lower())\n",
    "    for j in token:\n",
    "        if re.findall(fr\"\\b {word.lower()}[.?,!\"\"'']*$\",j):\n",
    "            print(j)\n",
    "            count+=1\n",
    "print(\"Number of sentences ending with the custom word are: \"+str(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As I have not made functions for the tasks 2,3,4 but instead done them in a cell, \n",
    "# I would not be able to call the functions and show the running code as deadline will pass \n",
    "# but I am mentioning the heuristic I will use:\n",
    "#  If emails number and money and capital all present: classify as spam.\n",
    "#  If money and phone or money and email present classify as spam.\n",
    "#  If phone or email mentioned classify as ham.\n",
    "# If money mentioned classify as spam.\n",
    "# else classify as ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task10(word,text):\n",
    "    word=word.lower()\n",
    "    sent=sent_tokenize(text.lower())\n",
    "    token=re.findall(fr\"\\b(^|[^a-zA-Z]){word.lower()}([^a-zA-Z]|$)\",text)\n",
    "    print(\"Count of word: \"+str(len(token)))\n",
    "    count_sent=0\n",
    "    for i in sent:\n",
    "        token=re.findall(fr\"\\b(^|[^a-zA-Z]){word.lower()}([^a-zA-Z]|$)\",i)\n",
    "        if(len(token)>0):\n",
    "            print(i)\n",
    "            count_sent+=1\n",
    "    \n",
    "    print(\"count of sentences: \"+str(count_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of word: 2\n",
      "hi!.\n",
      "hi hi hi .\n",
      "hi hi.\n",
      "count of sentences: 3\n"
     ]
    }
   ],
   "source": [
    "task10(\"hi\",\"Hi!. Hi hi hi . hi hi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
